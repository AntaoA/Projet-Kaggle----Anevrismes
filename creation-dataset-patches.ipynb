{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99552,"databundleVersionId":13851420,"sourceType":"competition"},{"sourceId":13236650,"sourceType":"datasetVersion","datasetId":8387937}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, h5py, numpy as np, pandas as pd, pydicom, json\nfrom tqdm import tqdm\n\nh5_path = \"/kaggle/input/rsna-dataset/dataset.h5\"\nseries_root = \"/kaggle/input/rsna-intracranial-aneurysm-detection/series\"\npatch_size = (64,64,64)\nstride = (64,64,64)   # stride = taille patch => pas de recouvrement\nout_path = \"/kaggle/working/patches.h5\"\n\ntrain_df = pd.read_csv(\"/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv\")\nlocalizers = pd.read_csv(\"/kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv\")\nlocalizers[\"coords\"] = localizers[\"coordinates\"].apply(eval)\n\n_sop2z_cache = {}\ndef build_sop2z(series_uid: str):\n    if series_uid in _sop2z_cache:\n        return _sop2z_cache[series_uid]\n    series_path = os.path.join(series_root, series_uid)\n    files = [f for f in os.listdir(series_path) if f.endswith(\".dcm\")]\n    rows = []\n    for f in files:\n        ds = pydicom.dcmread(os.path.join(series_path,f), stop_before_pixels=True, force=True)\n        inst = getattr(ds, \"InstanceNumber\", None)\n        sop = getattr(ds, \"SOPInstanceUID\", None)\n        rows.append((inst, sop))\n    rows = sorted(rows, key=lambda t: (t[0] if t[0] is not None else 0))\n    mapping = {str(sop): z for z, (_, sop) in enumerate(rows) if sop is not None}\n    _sop2z_cache[series_uid] = mapping\n    return mapping\n\n\n\ndef crop_pad_cube(vol, start, size):\n    \"\"\"Extrait un cube (D,H,W) à partir de start (z,y,x), pad si nécessaire pour obtenir size.\"\"\"\n    D,H,W = vol.shape\n    pd_,ph,pw = size\n    z,y,x = start\n\n    patch = vol[z:z+pd_, y:y+ph, x:x+pw]\n\n    pad_z = max(0, pd_ - patch.shape[0])\n    pad_y = max(0, ph  - patch.shape[1])\n    pad_x = max(0, pw  - patch.shape[2])\n\n    patch = np.pad(patch, ((0,pad_z),(0,pad_y),(0,pad_x)), mode=\"constant\")\n    return patch\n\n\n\n\nwith h5py.File(h5_path, \"r\") as f:\n    meta = f[\"meta\"][:]\nuids = [uid.decode() if isinstance(uid, bytes) else str(uid) for uid in meta[\"series_uid\"]]\nidxs = [int(i) for i in meta[\"h5_index\"]]\nuid2idx = {uid: idx for uid, idx in zip(uids, idxs)}\n\nprint(f\"[INFO] {len(uids)} séries trouvées dans HDF5.\")\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nwith h5py.File(out_path, \"w\") as out_f, h5py.File(h5_path, \"r\") as f:\n    gX = out_f.create_group(\"X\")\n    gy = out_f.create_group(\"y\")\n    gmeta = out_f.create_group(\"meta\")\n\n    patch_id = 0\n    margin = 5  # marge en voxels autour de l'anévrysme\n    count_pos, count_neg = 0, 0\n\n    pbar = tqdm(uids, desc=\"Découpage des séries\")\n    for uid in pbar:\n        if uid not in uid2idx:\n            continue\n\n        h5_idx = uid2idx[uid]\n        vol = f[\"X\"][str(h5_idx)][()]\n        if vol.ndim == 4 and vol.shape[0] == 1: \n            vol = vol[0]\n        elif vol.ndim == 4 and vol.shape[-1] == 1: \n            vol = vol[...,0]\n        if vol.ndim != 3: \n            continue\n\n        D,H,W = vol.shape\n\n        # --- récupérer annotations ---\n        annots = localizers[localizers[\"SeriesInstanceUID\"] == uid]\n        centers = []\n        if len(annots) > 0:\n            sop2z = build_sop2z(uid)\n            for _, row in annots.iterrows():\n                sop = str(row[\"SOPInstanceUID\"])\n                if sop not in sop2z: \n                    continue\n                cz = sop2z[sop]\n                cx = int(round(row[\"coords\"][\"x\"]))\n                cy = int(round(row[\"coords\"][\"y\"]))\n                centers.append((cz,cy,cx))\n\n        pd_, ph, pw = patch_size\n        sd, sh, sw = stride\n\n        for z in range(0, max(1,D-pd_+1), sd):\n            for y in range(0, max(1,H-ph+1), sh):\n                for x in range(0, max(1,W-pw+1), sw):\n                    patch = crop_pad_cube(vol, (z,y,x), patch_size)\n\n                    # ⚡ Stockage en uint8 = 1 octet par voxel (valeurs 0–7 déjà)\n                    patch = patch.astype(np.uint8)\n                    patch = np.expand_dims(patch, -1)  # (D,H,W,1)\n\n                    # Label = 1 si un centre tombe dans le patch (+ marge)\n                    label = 0\n                    for (cz,cy,cx) in centers:\n                        if (z - margin <= cz < z+pd_ + margin and\n                            y - margin <= cy < y+ph + margin and\n                            x - margin <= cx < x+pw + margin):\n                            label = 1\n                            break\n\n                    if label == 1:\n                        count_pos += 1\n                    else:\n                        count_neg += 1\n\n                    # Sauvegarde patch\n                    gX.create_dataset(str(patch_id), data=patch, compression=\"gzip\", compression_opts=4)\n                    gy.create_dataset(str(patch_id), data=np.array(label, dtype=np.uint8))\n                    gmeta.create_dataset(str(patch_id), \n                        data=np.string_(json.dumps({\n                            \"series_uid\": uid,\n                            \"patch_id\": patch_id,\n                            \"coords\": [z,y,x],\n                            \"label\": int(label)\n                        })))\n                    patch_id += 1\n\n                    # --- log tous les 100 patches ---\n                    if patch_id % 100 == 0:\n                        total = count_pos + count_neg\n                        pct = (count_pos / total * 100) if total > 0 else 0\n                        print(f\"[LOG] {patch_id} patches | pos={count_pos} | neg={count_neg} | %pos={pct:.2f}%\")\n\n    total = count_pos + count_neg\n    pct = (count_pos / total * 100) if total > 0 else 0\n    pbar.set_postfix({\n        \"patches\": total,\n        \"pos\": count_pos,\n        \"neg\": count_neg,\n        \"%pos\": f\"{pct:.2f}%\"\n    })\n\nprint(f\"✅ Fini. Sauvé {patch_id} patches dans {out_path}\")\nprint(f\"   Patches positifs : {count_pos}\")\nprint(f\"   Patches négatifs : {count_neg}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}